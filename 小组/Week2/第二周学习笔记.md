# 第二周学习笔记

### 1.最小二乘法 & 梯度下降法

**最小二乘法**：以一元函数为例，自变量为x，因变量为y，而函数的斜率就是我们要拟合的结果。那么如何拟合出最佳斜率？数学家推断出，所有（真实值－预测值）的平方和最小的时候，斜率即为最佳斜率。这就是最小二乘法的思想，其中的二乘指的便是平方。那么如何求出该最佳斜率？首先以斜率为未知数，那么它的导数为0时便是斜率最小的时候。

**梯度下降法**：而拓展到多元函数，情况就变得复杂了。导数不再是一维的，而变成了一个向量（对各个方向的斜率求偏导）。就好比爬山，每个地方的坡度（梯度）都不一样，于是梯度下降法诞生了。这种方法的核心在于通过不停迭代，每一次迭代都更新y值，然后在不同的y值的导数向量（即梯度）又不同，这样不断更新，直到最后梯度不再变化，y的值便确定了。此时就拟合出了各个方向的斜率值。而在算式中，有两个关键变量，一个是学习率α，一个是迭代次数。学习率一般是0.01，但是不同情况都得经过调试得到最好的学习率。学习率即步长，也就好比爬山时每次需要迈出的步伐大小。如果步伐太大，可能直接越过最低点，函数难以收敛。而如果步伐太小，最后难以到达最低点。迭代次数即数据的更新次数，这个比较好理解。

注：求出的导数向量是“上山”方向，将其添上符号才是下山的方向。

**总而言之**，最小二乘法就是一种思想，它可以拟合任意函数，线性回归只是其中一个比较简单而且也很常用的函数，所以讲最小二乘法基本都会以它为例，而梯度下降法也是线性回归的一种方法。

### 2.逻辑回归*（分类算法：classification）*

与线性回归相对，逻辑回归的输出在0~1之间，也就是说它的输出是一个事件发生的概率而不是一个具体的值，因此被更多的运用在分类上

通常来说，逻辑回归=线性回归上再套一个sigmoid函数

在线性回归中，我们通常会使用残差平方和来评估模型的性能，残差平方和越小性能越好。而在逻辑回归中我们使用交叉熵函数，用来度量两个分布之间的距离（越小越好）

好处：减少极端数据对整体的影响，使模型的整体决策准确度提高

缺点：只适用于线性分布，对于非线性分布它的拟合结果就很差

### 3.kmeans算法*("聚类")*

kmeans用于分类，属于无监督学习。通过给kmeans一个训练集，设定合理的参数，通过不断地归类并更新每个类的中心点，最后完整地进行分类。而后引入测试集，就可以根据测试集的数据直接进行归类。

优点：不用过分追求样本质量，类似于“四舍五入”

缺点：不追求样本质量，也就意味着它的性能没有有监督学习那么好

### 4.深度学习*（keras看不太懂，后面再去系统学习）*

1. 机器学习是使用算法分析数据、**从数据中学习**，然后对新数据做出确定或预测的实践。

2. 深度学习是机器学习的一个子领域，它使用受大脑神经网络结构和功能启发的算法。

   我们在深度学习中使用的神经网络并不是真正的生物神经网络。它们只是与生物神经网络共享一些特征，因此，我们将它们称为**人工神经网络（ANN）**

3. **人工神经网络**（ANN）是一种计算系统，由一系列称为神经元的连接单元组成，这些单元被组织成我们所说的层。

   ANN包含三种层：

   1. 输入层：输入数据的每个组件都有一个节点。
   2. 隐藏层：为每个隐藏层任意选择数量的节点。
   3. 输出层：每个可能的所需输出都有一个节点。

   前向传播：输入层-->隐藏层-->输出层

    ANN 中最基本的层：密集层（密集层的每个输出都是使用该层的每个输入来计算的）

4. 层的种类：

   密集层（将每个输入完全连接到其层内的每个输出）

   卷积层（通常用于处理图像数据的模型中）

   池化层

   循环层（用于处理时间序列数据的模型）

   归一化层

   ：每一种层都有不同的优势区间

5. 在人工神经网络中，激活函数是将节点的输入映射到其相应输出的函数。

   其中就包括了逻辑回归中的sigmoid函数，本质上是对输入进行加权，然后进行输出

6. 当我们训练模型时，我们基本上是在尝试解决优化问题。我们正在尝试优化模型中的权重。

   类比线性回归，训练就是找出各个特征的权重，即各个theta值

   损失函数：反应预测值与真实值的关系，常常用于计算权重，已经评估模型的性能是否优良（比较常见的是均方误差函数MSE）

7. 一旦数据集中的所有数据点都通过网络，我们就说一个纪元完成了。

   **纪元**是指在训练期间将整个数据集传递到网络的单次传递。

   此处还涉及到梯度、学习率等知识，上文已经提到了

8. 损失函数

   keras中可用：

   1. **均方误差（MSE)**
   2. 平均绝对误差
   3. 平均绝对百分比误差
   4. 均方对数误差
   5. 方形铰链
   6. 合页
   7. 分类铰链
   8. 洛科什
   9. **分类**交叉熵
   10. 稀疏分类交叉熵
   11. 二元交叉熵
   12. kullback_leibler_divergence
   13. 泊松
   14. 余弦邻近度

9. 验证集是一组与训练集分开的数据，用于在训练期间验证我们的模型。此验证过程有助于提供可以帮助我们调整超参数的信息。

   #### 